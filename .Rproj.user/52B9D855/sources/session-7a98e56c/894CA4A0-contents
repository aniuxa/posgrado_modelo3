# Análisis de texto (I)

En esta primera práctica veremos algunas acciones para importación de texto para su análisis, así como el manejo de diferentes fuentes, así como importación de tablas desde pdf.

## Paquetes

*NOTA* Además de estos paquetes, necesitamos descargar JAVA en nuestro sistema operativo

<https://www.java.com/en/download/>

Instaleremos algunos paquetes, no todos los usaremos en esta práctica pero para irlos instalando:

```{r}
#install.packages("tokenizers", dependencies=T)
# Este paquete no se pudo instalar con pacman::p_load
```

Para Windows

```{r}
# on 64-bit Windows
#remotes::install_github(c("ropensci/tabulizerjars", "ropensci/tabulizer"), INSTALL_opts = "--no-multiarch")

```

Para MAC
```{r}
remotes::install_github(c("ropensci/tabulizerjars", "ropensci/tabulizer"))
```

```{r}

if (!require("pacman")) install.packages("pacman") # instala pacman si se requiere

pacman::p_load(tidyverse, magrittr, tidytext,
               tm, 
               NLP, SnowballC, 
               wordcloud, ggwordcloud,
               quanteda, udpipe,
               igraph, ggraph, # para unos gráficos
               readxl, janitor, textrank,
               broom, epubr, pdftools, tesseract, tokenizers,
               rJava, tabulizerjars, tabulizer )

```

## Importar desde un archivo .txt

Vamos a importar el discurso que el presidente dio el 1 de julio: <https://lopezobrador.org.mx/2022/07/01/discurso-del-presidente-andres-manuel-lopez-obrador-en-4-ano-del-triunfo-democratico-historico/>

```{r}
amlo <- readLines("datos/text/20220107_amlo.txt")

```

Revisemos un poco este objeto

```{r}
summary(amlo)
```

Tenemos 113 párrafos. Al momento no tenemos más información que lo revisaremos en siguientes secciones

## Importar un epub

El proyecto Gutenberg tiene una selección de libros publicados sin problemas de derechos de Autor. Trabajaremos con los *Cuentos de Amor de Locura y de Muerte* de Horacio Quiroga

```{r}
epubr::epub_head("datos/text/quiroga.epub") # muestra lo primero 
```

Es una selección de cuentos. Revisemos un poco la meta-data:

```{r}
epubr::epub_meta("datos/text/quiroga.epub") # muestra el meta-data del libro

```

Hoy sí lo vamos a importar en nuestro ambiente:

```{r}
x <- epubr::epub("datos/text/quiroga.epub") # Importa todo el libro en el objeto x, pero no queremos todo
x
```

¿Dónde están los cuentos?

```{r}
glimpse(x)
```

Vemos que en realidad la última variable es una lista que adentro trae un objeto "tbl_df" de 9 x 4. Revisemos qué hay

```{r}
class(x$data)

x$data
```

Como es una lista, pero una lista de un solo elemento \[\[1\]\], vamos a consultarlo:

```{r}
x$data[[1]]
```

Esta es nuestra matriz de datos. Aquí podemos elegir una sección. Por ejemplo un cuento

```{r}
epub<-x$data[[1]]
class(epub)
```

Vamos a quedarnos con un solo cuento:

```{r}
ojos_sombrios<-epub %>% 
  filter(section=="id00249") %>%  # nos quedamos con el primer cuento
  select(text)

ojos_sombrios

class(ojos_sombrios)

```

Sigue teniendo formato de data.frame. Para poder usar algunos elementos necesitamos convertirlo a texto. Por eso lo vamos a "pegar"

```{r}
ojos_sombrios<-paste(ojos_sombrios$text) # lo volvemos caracter
class(ojos_sombrios)


```

Con esto ya podremos hacer muchas operaciones de aquí en adelante.

## Importar un pdf con`{pdftools}`

```{r}
dof4nov <- pdftools::pdf_text("datos/text/04112022-MAT.pdf")
dof4nov[6] 
class(dof4nov)
```

Para verlo mejor podemos usar el comando `cat()` de base para cada una de las "hojas"

```{r}
cat(dof4nov[6])
```

Qué pasa cuando nuestro pdf tiene datos como tablas, ¿no sería genial?

## Extrayendo tablas de un pdf conn `{tabulizer}`

Tenemos un pequeño extracto del PEF

```{r}
PEF <- pdftools::pdf_data("datos/text/tomo_1_fa.pdf")[[3]]
```

Este funciona cuando tenemos "tablas limpias"

```{r}
tab1 <- tabulizer::extract_tables("datos/text/tomo_1_fa.pdf", pages = 3)
tab2 <- tabulizer::extract_tables("datos/text/tomo_1_fa.pdf", pages = 4)

```

```{r}
tabla1<-as_tibble(tab1[[1]])
tabla2<-as_tibble(tab2[[1]])

```

Vamos a juntar estas tablas:
```{r}
pef2022<-as_tibble(rbind(tabla1[-1,], 
                         tabla2[-1,])) #no queremos las primeras filas


```

Vamos a limpiar estos datos:
```{r}
names(pef2022)<-c("objeto", "importe")

```

Hoy veamos que nuestro importe está
```{r}
class(pef2022$importe)
```
Un problema es para convertirlo al número como tiene comas si hacemos un "as.numeric" lo hará NA
```{r}
pef2022 %<>% 
  mutate(importe2=as.numeric(importe))
```

¿Qué hacer?

Primero vamos a eliminar todas las comas de nuestra columna.. introduciremos un poco el paquete `{stringr}`

```{r}
pef2022 %<>% 
  mutate(importe2=stringr::str_remove_all(importe,pattern=",")) %>% 
  mutate(importe2=as.numeric(importe2))

```

Supongamos que queremos graficar los objetos que tengan que ver con "servicios"

```{r}
pef2022 %>% 
  filter(stringr::str_detect(objeto, # variable de caracter
                             pattern="Servicios")) %>% # patrón que buscamos que tenga
  ggplot()+
  aes(x=objeto, y=round(importe2/1000000,2)) +
  geom_bar(stat="identity") + coord_flip()
```



## Importar una imagen con texto con `{tesseract}`

El paquete `{tesseract}` ... 

> "utiliza datos de entrenamiento para realizar OCR. La mayoría de los sistemas utilizan de forma predeterminada los datos de entrenamiento en inglés". Para mejorar el rendimiento de OCR para otros idiomas, puede instalar los datos de entrenamiento de su distribución... En Windows y MacOS, puede instalar idiomas mediante la función tesseract_download, que descarga datos de entrenamiento directamente desde github y los almacena en la ruta del disco..." (traducido de la viñeta)

```{r}
if(is.na(match("spa", tesseract::tesseract_info()$available)))
  tesseract::tesseract_download("spa") # baja el entrenamiento para español

spa <- tesseract::tesseract("spa") # aquí este será el "engine"

text <- tesseract::ocr("datos/text/texto1.png", #ruta donde está la imagen
                       engine = spa) # que lo lea en español
cat(text)

```

## `{stringr}` Limpieza de variables de cadena

```{r}
stringr::str_squish(text)
```

```{r}
nota<-stringr::str_split_fixed(text, "\n\n", n=10) ## por párrafos
nota
```

```{r}
stringr::str_count(text, "\n\n")
```

```{r}
nota<-stringr::str_split_fixed(text,
                                 pattern="\n\n", 
                                 n=str_count(text, "\n\n")+1) ## por párrafos
nota
```

```{r}
stringr::str_squish(nota)
```

Si queremos quitar lo "-", lo podemos hacer:

```{r}
nota<-stringr::str_squish(nota)

stringr::str_remove_all(nota, 
                        pattern="- ")

```

## Más operaciones con cadenas con `{stringr}` y `{tokenizers}`

Ya vimos el conteo de algunos patrones y cómo podemos quitar algunos. Trabajemos con el cuento de Quiroga, que también está un poco sucio, y veamos como podemos seguir utilizando el formato tidy

```{r}
ojos_sombrios<-ojos_sombrios %>% 
  stringr::str_split_fixed( pattern="\n\n", n=str_count(text, "\n\n")+1) %>% ## por párrafos %
  stringr::str_squish()

```

Hay un personaje que se llama Nébel, veámos cuantas veces aparece:

```{r}
summary(ojos_sombrios)

ojos_sombrios %>% 
stringr::str_count(pattern="Vezzera") %>% 
  sum()
```

Vamos a ver cuántas palabras tiene cada párrafos, hay unos párrafos vacíos:

```{r}
tokenizers::count_words(ojos_sombrios)
```

Revisemos el discurso de Amlo:

```{r}
tokenizers::count_words(amlo)

```

De los 113 párrafos tenemos varios que están en 0. Vamos a eliminarlos. Vemos que txt reconoció los párrafos sin problemas, sin necesidad de poner la marca de párrafo

```{r}
count<-tokenizers::count_words(amlo) 
count==0

amlo<-amlo[!count==0] 
```

Vamos a jugar más con algunas opciones de `{tokenizers}`

```{r}
tokenizers::count_words(amlo)
tokenizers::count_sentences(amlo)
```


## Tokenización para análisis de texto

Las palabras tienen un papel en el lenguaje, por lo cual muchas veces la unidad que usaremos será esa. Uno de los primeros pasos para el análisis de texto será descomponer nuestros textos en palabras. 

### Tokenización con `{tidytext}`

Para usar `{tidytext}`, necesitamos que nuestro texto esté en formato tibble:
```{r}
amlo_df <-tibble(text=amlo)
```

A partir de esto podemos pasar al proceso de tokenización:

```{r}
amlo_df %>%
  unnest_tokens(word, text)
```

Podemos hacer un tabulado de estos elementos:

```{r}
amlo_df %>%
  tidytext::unnest_tokens(word, text) %>% 
  tabyl(word) %>% 
  arrange(-n) %>% 
  head(10)

```
Importa el tipo de palabra!!! En muchos idiomas las preposiciones y determinantes son bastante comunes. Hay varios diccionarios, incluso los podemos modificar. Para este ejercicio utilizaremos las palabras comunes del paquete `{quanteda}`

```{r}
quanteda::stopwords(language="spa")
stop<-quanteda::stopwords(language="spa")
```


```{r}
amlo_df %>%
  tidytext::unnest_tokens(word, text) %>% 
  filter(!word%in%stop) %>%  # ojo con el filtro
  tabyl(word) %>% 
  arrange(-n) %>% 
  head(10)

```

### Tokenización con `{udpipe}`

Para mayor información consultar
<https://ufal.mff.cuni.cz/~straka/papers/2017-conll_udpipe.pdf>

Primero vamos a bajar nuestro modelo en español
```{r}
udmodel <- udpipe_download_model(language = "spanish")  # esto trabaja con la estructura del español

```

Vamos a "tokenizar" el discurso de AMLO:

```{r}
amlo_udpipe<-udpipe(x =amlo, 
                      object=udmodel) #"tokeniza" el texto


```

Además de separarnos las palabras, también distingue puntuación (por eso tiene más líneas que las palabras), pero también nos da las "Universal POS tags", donde POS=part-of-speech, que están en la variable "upos"

```{r}
amlo_udpipe %>% 
  head(10)
```

¿Con cuales UPOS se trabaja? <https://universaldependencies.org/u/pos/>

```{r}
amlo_udpipe %>% 
  tabyl(upos)
```

¿Qué no logró identificar?

```{r}
amlo_udpipe %>% 
  filter(is.na(upos)) %>% 
  tabyl(token)
```

La clasificación de las UPOS nos permite hacer filtro por el tipo de palabra que queremos analizar, seguro los sustantivos son los que más queremos revisar:


```{r}
amlo_udpipe %>% 
  filter(upos=="NOUN") %>% 
  tabyl(token) %>% 
  arrange(-n) %>% 
  head(10)
```

Además de los "tokens" podemos pedirles los "lemma", que como vemos quita el género y el número
```{r}
amlo_udpipe %>% 
  filter(upos=="NOUN") %>% 
  tabyl(lemma) %>% 
  arrange(-n) %>% 
  head(10)
```